{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69bf32ac-0b2d-4ccf-9648-a9d43022ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "200c6e5a-dd2a-400d-b6cb-fe79d56f99c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/5zh9t8v14h9c19sssjwwlz940000gp/T/ipykernel_41291/4225091663.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv('data/original/total.csv', sep=\",\", error_bad_lines=False, encoding= \"unicode_escape\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/original/total.csv', sep=\",\", error_bad_lines=False, encoding= \"unicode_escape\")\n",
    "test_data = pd.read_csv('data/test/sentiment-topic-final-test.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35adf7-0046-42b6-86ca-3c32f2c34964",
   "metadata": {},
   "source": [
    "## Conventional Machine Learning Approach: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12aaca35-22e8-46f1-b1eb-936ec5ca0d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/irem.demir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/irem.demir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/opt/anaconda3/envs/text-mining/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# vectorizer taken from previous labs setup\n",
    "count_vectorizer = CountVectorizer(min_df=2, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "\n",
    "\n",
    "\n",
    "counts = count_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf = tfidf_transformer.fit_transform(counts)\n",
    "\n",
    "# split the data into train and dev\n",
    "docs_train, docs_dev, y_train, y_dev = train_test_split(\n",
    "    tfidf, # the tf-idf model\n",
    "    df['topic'], # the labels\n",
    "    test_size=0.1, # the size of the dev set\n",
    "    random_state=0, # random seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa7ef89-7e75-485f-a8d0-0b5d1e68fdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Accuracy: 0.9677777777777777\n",
      "Dev report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        book       0.97      0.97      0.97       286\n",
      "       movie       0.95      0.97      0.96       307\n",
      "  restaurant       0.99      0.97      0.98       307\n",
      "\n",
      "    accuracy                           0.97       900\n",
      "   macro avg       0.97      0.97      0.97       900\n",
      "weighted avg       0.97      0.97      0.97       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM classifier from sklearn\n",
    "\n",
    "# train the model\n",
    "svm = LinearSVC()\n",
    "svm.fit(docs_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "y_pred = svm.predict(docs_dev)\n",
    "\n",
    "print(\"Dev Accuracy:\", accuracy_score(y_dev, y_pred))\n",
    "print('Dev report:\\n')\n",
    "\n",
    "print(classification_report(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9585b0-637d-4ca1-917b-a8cc81b73d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9\n",
      "Test report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        book       1.00      1.00      1.00         2\n",
      "       movie       0.83      1.00      0.91         5\n",
      "  restaurant       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.94      0.89      0.90        10\n",
      "weighted avg       0.92      0.90      0.89        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "docs_test = count_vectorizer.transform(test_data['text'])\n",
    "docs_test = tfidf_transformer.transform(docs_test)\n",
    "\n",
    "y_pred = svm.predict(docs_test)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(test_data['topic'], y_pred))\n",
    "\n",
    "print('Test report:\\n')\n",
    "print(classification_report(test_data['topic'], y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c4637b7",
   "metadata": {},
   "source": [
    "# TRANSFORMER: ROBERTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37ae1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, dev = train_test_split(df, test_size=0.1, random_state=1, \n",
    "                               stratify=df[['topic']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c519647b-ea75-4a2b-bf2a-f2af5806afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformer based topic classifier for 3 topics\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "model_args = ClassificationArgs()\n",
    "\n",
    "model_args.overwrite_output_dir=True # overwrite existing saved models in the same directory\n",
    "model_args.evaluate_during_training=True # to perform evaluation while training the model\n",
    "# (eval data should be passed to the training method)\n",
    "\n",
    "model_args.num_train_epochs=10 # number of epochs\n",
    "model_args.train_batch_size=32 # batch size\n",
    "model_args.learning_rate=4e-6 # learning rate\n",
    "model_args.max_seq_length=256 # maximum sequence length\n",
    "# Note! Increasing max_seq_len may provide better performance, but training time will increase. \n",
    "# For educational purposes, we set max_seq_len to 256.\n",
    "\n",
    "# Early stopping to combat overfitting: https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping\n",
    "model_args.use_early_stopping=True\n",
    "model_args.early_stopping_delta=0.01 # \"The improvement over best_eval_loss necessary to count as a better checkpoint\"\n",
    "model_args.early_stopping_metric='eval_loss'\n",
    "model_args.early_stopping_metric_minimize=True\n",
    "model_args.early_stopping_patience=2\n",
    "model_args.evaluate_during_training_steps=32 # how often you want to run validation in terms of training steps (or batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5205de7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 440M/440M [01:19<00:00, 5.57MB/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create a ClassificationModel\n",
    "model = ClassificationModel('bert', 'bert-base-uncased', num_labels=3, args=model_args, use_cuda=False)\n",
    "\n",
    "# Fine-tune the model\n",
    "model.train_model(train)\n",
    "\n",
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(dev)\n",
    "\n",
    "# print the results\n",
    "print(result)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(dev['topic'], model_outputs.argmax(axis=1)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaad3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8382cef7e68f4f856d7dd605fa8d3bfcec002265d1363facf2e5b61f7575780d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
